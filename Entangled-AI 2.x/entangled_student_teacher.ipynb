{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6554ac9",
   "metadata": {},
   "source": [
    "# üìò Entangled Learning: Teacher-Student with PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608374ef",
   "metadata": {},
   "source": [
    "\n",
    "Bu not defteri, `entangled learning` kavramƒ±nƒ±n **√∂ƒüretmen-√∂ƒürenci** mimarisinde nasƒ±l i≈üe yaradƒ±ƒüƒ±nƒ± g√∂steren k√º√ß√ºk bir prototip i√ßerir.\n",
    "\n",
    "- **√ñƒüretmen (MLP_Large)**: Orijinal veriyle eƒüitilir\n",
    "- **√ñƒürenci (MLP_Small)**: PCA ile sadele≈ütirilmi≈ü veriyle √ßalƒ±≈üƒ±r\n",
    "- Ama√ß: √ñƒürenci, hem etiketlere hem de √∂ƒüretmenin √ßƒ±ktƒ±sƒ±na benzemeye √ßalƒ±≈üƒ±r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624a7bfc",
   "metadata": {},
   "source": [
    "## üìä Veri Hazƒ±rlƒ±ƒüƒ±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85eb2952",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "X, y = make_classification(n_samples=3000, n_features=20, n_informative=15, random_state=42)\n",
    "X = StandardScaler().fit_transform(X)\n",
    "X_pca = PCA(n_components=10).fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test, X_pca_train, X_pca_test = train_test_split(X, y, X_pca, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a018bf54",
   "metadata": {},
   "source": [
    "## üß† Model Tanƒ±mlarƒ±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e23c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "def make_mlp(input_shape, units_list):\n",
    "    model = Sequential()\n",
    "    for i, u in enumerate(units_list):\n",
    "        if i == 0:\n",
    "            model.add(Dense(u, activation='relu', input_shape=(input_shape,)))\n",
    "        else:\n",
    "            model.add(Dense(u, activation='relu'))\n",
    "        model.add(Dropout(0.3))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "mlp_large = make_mlp(X.shape[1], [64, 32])\n",
    "mlp_small = make_mlp(X_pca.shape[1], [32])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3642c5c4",
   "metadata": {},
   "source": [
    "## üîÅ Entangled Eƒüitim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90019729",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "\n",
    "optimizer_large = tf.keras.optimizers.Adam()\n",
    "optimizer_small = tf.keras.optimizers.Adam()\n",
    "bce = BinaryCrossentropy()\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "history = {\"loss_large\": [], \"loss_small\": [], \"kl_large\": [], \"kl_small\": [], \"lambda\": []}\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    Œª = 0.05 * np.log1p(epoch) / np.log1p(epochs - 1)  # logaritmik artƒ±≈ü\n",
    "    history[\"lambda\"].append(Œª)\n",
    "\n",
    "    idxs = np.random.permutation(len(X_train))\n",
    "    X_t, X_p, y_t = X_train[idxs], X_pca_train[idxs], y_train[idxs]\n",
    "    loss_l_all, loss_s_all, kl_l_all, kl_s_all = [], [], [], []\n",
    "\n",
    "    for i in range(0, len(X_t), batch_size):\n",
    "        xb, xpb, yb = X_t[i:i+batch_size], X_p[i:i+batch_size], y_t[i:i+batch_size]\n",
    "\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            yl = mlp_large(xb, training=True)\n",
    "            ys = mlp_small(xpb, training=True)\n",
    "            yl_np, ys_np = yl.numpy().flatten(), ys.numpy().flatten()\n",
    "\n",
    "            ce_l = bce(yb, yl)\n",
    "            ce_s = bce(yb, ys)\n",
    "            kl_l = tf.reduce_mean(yl_np * tf.math.log(tf.clip_by_value(yl_np / ys_np, 1e-7, 1e7)))\n",
    "            kl_s = tf.reduce_mean(ys_np * tf.math.log(tf.clip_by_value(ys_np / yl_np, 1e-7, 1e7)))\n",
    "            loss_l = ce_l + Œª * kl_l\n",
    "            loss_s = ce_s + Œª * kl_s\n",
    "\n",
    "        grads_l = tape.gradient(loss_l, mlp_large.trainable_weights)\n",
    "        grads_s = tape.gradient(loss_s, mlp_small.trainable_weights)\n",
    "        optimizer_large.apply_gradients(zip(grads_l, mlp_large.trainable_weights))\n",
    "        optimizer_small.apply_gradients(zip(grads_s, mlp_small.trainable_weights))\n",
    "\n",
    "        loss_l_all.append(loss_l.numpy())\n",
    "        loss_s_all.append(loss_s.numpy())\n",
    "        kl_l_all.append(kl_l.numpy())\n",
    "        kl_s_all.append(kl_s.numpy())\n",
    "\n",
    "    history[\"loss_large\"].append(np.mean(loss_l_all))\n",
    "    history[\"loss_small\"].append(np.mean(loss_s_all))\n",
    "    history[\"kl_large\"].append(np.mean(kl_l_all))\n",
    "    history[\"kl_small\"].append(np.mean(kl_s_all))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23afea82",
   "metadata": {},
   "source": [
    "## üìà Sonu√ßlarƒ±n G√∂rselle≈ütirilmesi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b24e019",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(history[\"loss_large\"], label=\"MLP_Large Loss\")\n",
    "plt.plot(history[\"loss_small\"], label=\"MLP_Small Loss\")\n",
    "plt.plot(history[\"kl_large\"], '--', label=\"KL(Large || Small)\")\n",
    "plt.plot(history[\"kl_small\"], '--', label=\"KL(Small || Large)\")\n",
    "plt.plot(history[\"lambda\"], label=\"Œª (Entanglement)\", color='gray', linestyle=':')\n",
    "plt.title(\"Bidirectional Entangled Learning (X vs PCA(X))\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss / KL\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6459a6c9",
   "metadata": {},
   "source": [
    "## üßæ Final Epoch √ñzeti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f082af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f\"Last Epoch Œª (Entanglement): {history['lambda'][-1]:.4f}\")\n",
    "print(f\"MLP_Large Final Loss: {history['loss_large'][-1]:.4f}\")\n",
    "print(f\"MLP_Small Final Loss: {history['loss_small'][-1]:.4f}\")\n",
    "print(f\"KL(Large || Small) Final: {history['kl_large'][-1]:.4f}\")\n",
    "print(f\"KL(Small || Large) Final: {history['kl_small'][-1]:.4f}\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
